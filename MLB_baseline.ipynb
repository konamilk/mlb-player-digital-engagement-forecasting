{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLB_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jF6kZTC0u2MaPwrFzh4L-lG0kKBqZR2w",
      "authorship_tag": "ABX9TyNhuUxkRjUZwoy3ptMOye7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konamilk/mlb-player-digital-engagement-forecasting/blob/main/MLB_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BZU8bSwR319T",
        "outputId": "57fd2710-757b-4078-ede0-83ec73f96d6b"
      },
      "source": [
        "from requests import get\n",
        "NOTEBOOK = get('http://172.28.0.2:9000/api/sessions').json()[0]['name'].split('.')[0]\n",
        "NOTEBOOK"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MLB_baseline'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtjMBhxrr7X8"
      },
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "if 'google.colab' in sys.modules:\n",
        "    INPUT = Path('/content/input/')\n",
        "elif 'kaggle_web_client' in sys.modules:\n",
        "    INPUT = Path('../input/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0JWJatJshsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa32721-58e2-4fad-c1ae-1dab98fa3e80"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  !mkdir ~/.kaggle\n",
        "  !cp /content/drive/MyDrive/.kaggle/kaggle.json ~/.kaggle\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "  !pip install kaggle\n",
        "  !pip install git+https://github.com/pfnet-research/xfeat.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Collecting git+https://github.com/pfnet-research/xfeat.git\n",
            "  Cloning https://github.com/pfnet-research/xfeat.git to /tmp/pip-req-build-m_eefw5i\n",
            "  Running command git clone -q https://github.com/pfnet-research/xfeat.git /tmp/pip-req-build-m_eefw5i\n",
            "Requirement already satisfied (use --upgrade to upgrade): xfeat==0.1.1 from git+https://github.com/pfnet-research/xfeat.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (3.13)\n",
            "Requirement already satisfied: ml_metrics in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (0.1.4)\n",
            "Requirement already satisfied: optuna>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (2.8.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (0.22.2.post1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from xfeat==0.1.1) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ml_metrics->xfeat==0.1.1) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ml_metrics->xfeat==0.1.1) (1.19.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (1.4.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (1.6.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (1.4.18)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (5.0.1)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (4.41.1)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna>=1.3.0->xfeat==0.1.1) (3.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->xfeat==0.1.1) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ml_metrics->xfeat==0.1.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ml_metrics->xfeat==0.1.1) (2.8.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=1.3.0->xfeat==0.1.1) (1.1.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna>=1.3.0->xfeat==0.1.1) (1.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=1.3.0->xfeat==0.1.1) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna>=1.3.0->xfeat==0.1.1) (4.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna>=1.3.0->xfeat==0.1.1) (2.4.7)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=1.3.0->xfeat==0.1.1) (5.6.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=1.3.0->xfeat==0.1.1) (2.1.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=1.3.0->xfeat==0.1.1) (2.1.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna>=1.3.0->xfeat==0.1.1) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ml_metrics->xfeat==0.1.1) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna>=1.3.0->xfeat==0.1.1) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna>=1.3.0->xfeat==0.1.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna>=1.3.0->xfeat==0.1.1) (3.7.4.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna>=1.3.0->xfeat==0.1.1) (0.2.5)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=1.3.0->xfeat==0.1.1) (0.4.4)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=1.3.0->xfeat==0.1.1) (1.8.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna>=1.3.0->xfeat==0.1.1) (21.2.0)\n",
            "Building wheels for collected packages: xfeat\n",
            "  Building wheel for xfeat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xfeat: filename=xfeat-0.1.1-cp37-none-any.whl size=39635 sha256=d77e7d4596f99f6099242746268f34bcc80a509df7affcfbdde69987c333cb8a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hj0qu4ir/wheels/82/f3/da/060c6ceac1125aa285b041284b7ec5324be345b865c713da9c\n",
            "Successfully built xfeat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l02aDXrawzZw",
        "outputId": "1f5e8532-2853-43e2-bd0f-45418d2c46e2"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  !kaggle competitions download -c mlb-player-digital-engagement-forecasting\n",
        "  !mkdir input\n",
        "  !unzip -o '*.zip' -d ./input/\n",
        "  !rm *.zip\n",
        "  !mv *.csv ./input/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "competition.cpython-37m-x86_64-linux-gnu.so: Skipping, found more recently modified local copy (use --force to force download)\n",
            "__init__.py: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Downloading teams.csv to /content\n",
            "  0% 0.00/3.68k [00:00<?, ?B/s]\n",
            "100% 3.68k/3.68k [00:00<00:00, 3.63MB/s]\n",
            "Downloading awards.csv to /content\n",
            "  0% 0.00/820k [00:00<?, ?B/s]\n",
            "100% 820k/820k [00:00<00:00, 54.2MB/s]\n",
            "Downloading players.csv to /content\n",
            "  0% 0.00/173k [00:00<?, ?B/s]\n",
            "100% 173k/173k [00:00<00:00, 56.8MB/s]\n",
            "Downloading example_sample_submission.csv to /content\n",
            "  0% 0.00/191k [00:00<?, ?B/s]\n",
            "100% 191k/191k [00:00<00:00, 62.3MB/s]\n",
            "Downloading seasons.csv to /content\n",
            "  0% 0.00/824 [00:00<?, ?B/s]\n",
            "100% 824/824 [00:00<00:00, 3.02MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 96% 452M/470M [00:02<00:00, 148MB/s]\n",
            "100% 470M/470M [00:03<00:00, 157MB/s]\n",
            "Downloading example_test.csv.zip to /content\n",
            "  0% 0.00/3.96M [00:00<?, ?B/s]\n",
            "100% 3.96M/3.96M [00:00<00:00, 64.5MB/s]\n",
            "mkdir: cannot create directory ‘input’: File exists\n",
            "Archive:  train.csv.zip\n",
            "  inflating: ./input/train.csv       \n",
            "\n",
            "Archive:  example_test.csv.zip\n",
            "  inflating: ./input/example_test.csv  \n",
            "\n",
            "2 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSURoFztw90s"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.utils import data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Sk6gVO7JkG"
      },
      "source": [
        "TRAIN = INPUT/'train.csv'\n",
        "TEAMS = INPUT/'awards.csv'\n",
        "PLAYERS = INPUT/'players.csv'\n",
        "AWARDS = INPUT/'awards.csv'\n",
        "SEASONS = INPUT/'seasons.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh1fdI--UN4X"
      },
      "source": [
        "df = pd.read_csv(TRAIN)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61DsO9JyHKoI"
      },
      "source": [
        "def unpack_json(json_str):\n",
        "    return np.nan if pd.isna(json_str) else pd.read_json(json_str)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOOU2R0YfZRv"
      },
      "source": [
        "import datetime\n",
        "def next_date_as_int(date_as_int: int):\n",
        "  dd = date_as_int % 100\n",
        "  date_as_int = date_as_int // 100\n",
        "  mm = date_as_int % 100\n",
        "  yyyy = date_as_int // 100\n",
        "  dt = datetime.datetime(yyyy, mm, dd)\n",
        "  next_dt = dt + datetime.timedelta(days=1)\n",
        "  return next_dt.year * 10000 + next_dt.month * 100 + next_dt.day"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_7aEDMgva7c"
      },
      "source": [
        "class TrainDataset(object):\n",
        "  def __init__(self,df_train, batch_size=5):\n",
        "    self.df_train = df_train\n",
        "    self.current = 0\n",
        "    self.batch_size = batch_size\n",
        "    self.df_example_sample_submission = pd.read_csv(INPUT/'example_sample_submission.csv')\n",
        "    self.playerId =  self.df_example_sample_submission[self.df_example_sample_submission.date == self.df_example_sample_submission.date[0]].date_playerId.map(lambda x: int(x.split('_')[1]))\n",
        "    self.playerId.name = 'playerId'\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    start = self.current * self.batch_size\n",
        "    end = (self.current + 1) * self.batch_size\n",
        "    self.current += 1\n",
        "\n",
        "    if start >= self.df_train.shape[0]:\n",
        "      raise StopIteration()\n",
        "    \n",
        "    dates = self.df_train[start:end].date.unique()\n",
        "\n",
        "    temps = []\n",
        "    for date in dates:\n",
        "      df_temp = pd.DataFrame(self.playerId)\n",
        "      df_temp['date'] = date\n",
        "      df_temp['date_playerId'] = str(next_date_as_int(date)) + '_' + df_temp['playerId'].astype(str)\n",
        "      df_temp['target1'] = 0.0\n",
        "      df_temp['target2'] = 0.0\n",
        "      df_temp['target3'] = 0.0\n",
        "      df_temp['target4'] = 0.0\n",
        "      df_temp = df_temp.drop(['playerId'], axis=1)\n",
        "      temps.append(df_temp)\n",
        "\n",
        "    return  self.df_train[start:end].drop('nextDayPlayerEngagement', axis=1), pd.concat(temps, axis=0), self.df_train[start:end].nextDayPlayerEngagement"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLTd2Z7uNqEL"
      },
      "source": [
        "from xfeat import SelectCategorical, LabelEncoder, Pipeline, ConcatCombination, SelectNumerical, \\\n",
        "    ArithmeticCombinations, TargetEncoder, aggregation, GBDTFeatureSelector, GBDTFeatureExplorer\n",
        "\n",
        "class PreProcessor(object):\n",
        "  def __init__(self):\n",
        "    self.label_encoder = Pipeline([\n",
        "      SelectCategorical(exclude_cols=['engagementMetricsDate']),\n",
        "      LabelEncoder(output_suffix=\"\"),\n",
        "      ])\n",
        "\n",
        "  def pre_process(self, df_dairy: pd.DataFrame, df_submission: pd.DataFrame, phase = \"train\") -> pd.DataFrame:\n",
        "\n",
        "    X = df_submission.drop(['target1', 'target2', 'target3', 'target4'], axis=1)\n",
        "    X['date'] = pd.to_datetime(X['date'].astype(str))\n",
        "    X['year'] = X['date'].dt.year\n",
        "    X['month'] = X['date'].dt.month\n",
        "    X['day'] = X['date'].dt.day\n",
        "    X['dayofweek'] = X['date'].dt.dayofweek\n",
        "    X['engagementMetricsDate'] = pd.to_datetime(X['date_playerId'].str.split('_', expand = True)[0])\n",
        "    X['playerId'] = X['date_playerId'].str.split('_', expand = True)[1].astype(int)\n",
        "\n",
        "    # join rosters\n",
        "    df_rosters = pd.concat(map(lambda x: unpack_json(x), df_dairy[df_dairy['rosters'].notnull()]['rosters']), axis=0)\n",
        "    df_rosters['gameDate'] = pd.to_datetime(df_rosters['gameDate'])\n",
        "    X = pd.merge(X, df_rosters, how='left', left_on=['date', 'playerId'], right_on=['gameDate', 'playerId']).drop(['gameDate','status'], axis=1)\n",
        "    del df_rosters\n",
        "\n",
        "    # # join player twitter followers\n",
        "    # df_playerTwitterFollowers = pd.concat(map(lambda x: unpack_json(x),df_dairy[df_dairy['playerTwitterFollowers'].notnull()]['playerTwitterFollowers']),axis=0)\n",
        "    # df_playerTwitterFollowers = df_playerTwitterFollowers[['date', 'playerId', 'numberOfFollowers']]\n",
        "    # df_playerTwitterFollowers['year'] = df_playerTwitterFollowers['date'].dt.year\n",
        "    # df_playerTwitterFollowers['month'] = df_playerTwitterFollowers['date'].dt.month\n",
        "    # df_playerTwitterFollowers = df_playerTwitterFollowers.drop('date', axis=1)\n",
        "    # X = pd.merge(X, df_playerTwitterFollowers, how='left', on=['playerId', 'year', 'month'])\n",
        "    # del df_playerTwitterFollowers\n",
        "\n",
        "    # # join team twitter followers\n",
        "    # df_teamTwitterFollowers = pd.concat(map(lambda x: unpack_json(x),df_dairy[df_dairy['teamTwitterFollowers'].notnull()]['teamTwitterFollowers']),axis=0)\n",
        "    # df_teamTwitterFollowers = df_teamTwitterFollowers[['date', 'teamId', 'numberOfFollowers']]\n",
        "    # df_teamTwitterFollowers['year'] = df_teamTwitterFollowers['date'].dt.year\n",
        "    # df_teamTwitterFollowers['month'] = df_teamTwitterFollowers['date'].dt.month\n",
        "    # df_teamTwitterFollowers = df_teamTwitterFollowers.drop('date', axis=1)\n",
        "    # X = pd.merge(X, df_teamTwitterFollowers, how='left', on=['teamId', 'year', 'month'])\n",
        "    # del df_teamTwitterFollowers\n",
        "\n",
        "    # # join games\n",
        "    # df_games = pd.concat(map(lambda x: unpack_json(x),df_dairy[df_dairy['games'].notnull()]['games']),axis=0)\n",
        "    # new_columns = ['gamePk', 'gameType', 'season', 'gameDate', 'gameTimeUTC', 'resumeDate',\n",
        "    #       'resumedFrom', 'codedGameState', 'detailedGameState', 'isTie',\n",
        "    #       'gameNumber', 'doubleHeader', 'dayNight', 'scheduledInnings',\n",
        "    #       'gamesInSeries', 'seriesDescription', 'teamId', 'teamName',\n",
        "    #       'teamAbbrev', 'teamWins', 'teamLosses', 'teamWinPct', 'teamWinner',\n",
        "    #       'teamScore', 'opponentId', 'opponentName', 'opponentAbbrev', 'opponentWins',\n",
        "    #       'opponentLosses', 'opponentWinPct', 'opponentWinner', 'opponentScore']\n",
        "    # df_games_home = df_games.copy()\n",
        "    # df_games_home.columns = new_columns\n",
        "    # df_games_away = df_games.copy()[['gamePk', 'gameType', 'season', 'gameDate', 'gameTimeUTC', 'resumeDate',\n",
        "    #       'resumedFrom', 'codedGameState', 'detailedGameState', 'isTie',\n",
        "    #       'gameNumber', 'doubleHeader', 'dayNight', 'scheduledInnings',\n",
        "    #       'gamesInSeries', 'seriesDescription', 'awayId', 'awayName', 'awayAbbrev', 'awayWins',\n",
        "    #       'awayLosses', 'awayWinPct', 'awayWinner', 'awayScore', 'homeId', 'homeName',\n",
        "    #       'homeAbbrev', 'homeWins', 'homeLosses', 'homeWinPct', 'homeWinner',\n",
        "    #       'homeScore']]\n",
        "    # df_games_away.columns = new_columns\n",
        "    # df_games_2 = pd.concat([df_games_home, df_games_away], axis=0).drop(['teamName', 'teamAbbrev', 'opponentName', 'opponentAbbrev', 'opponentWinner'], axis=1)\n",
        "    # df_games_2['date'] = pd.to_datetime(df_games_2['gameDate'])\n",
        "    # X = pd.merge(X, df_games_2, how='left', on=['teamId', 'date'])\n",
        "    # del new_columns, df_games, df_games_home, df_games_away, df_games_2\n",
        "\n",
        "\n",
        "    # label encoding\n",
        "    if phase == 'train':\n",
        "      X = X.drop(['date', 'date_playerId'], axis=1)\n",
        "      encoded_X = self.label_encoder.fit_transform(X)\n",
        "      X = pd.concat([X['engagementMetricsDate'], encoded_X, SelectNumerical().fit_transform(X)], axis=1)\n",
        "    elif phase == 'eval':\n",
        "      X = X.drop(['date', 'date_playerId'], axis=1)\n",
        "      encoded_X = self.label_encoder.transform(X)\n",
        "      X = pd.concat([X['engagementMetricsDate'], encoded_X, SelectNumerical().fit_transform(X)], axis=1)\n",
        "    \n",
        "    return X\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzmCDV4B1Qeh"
      },
      "source": [
        "# make Traing\n",
        "df_train = df[(df.date >= 20200401) & (df.date <= 20210331)]\n",
        "# df_train = df[df.date <= 20210331]\n",
        "dts = TrainDataset(df_train, df_train.shape[0])\n",
        "\n",
        "df_dairy, df_submission, targets = next(dts)\n",
        "\n",
        "pre = PreProcessor()\n",
        "\n",
        "X = pre.pre_process(df_dairy, df_submission)\n",
        "\n",
        "# join targets\n",
        "df_targets = pd.concat(map(lambda z: unpack_json(z), targets), axis=0)\n",
        "df_targets['engagementMetricsDate'] = pd.to_datetime(df_targets['engagementMetricsDate'])\n",
        "y = pd.merge(X[['engagementMetricsDate', 'playerId']], df_targets, on=['engagementMetricsDate', 'playerId'], how='left')\n",
        "del df_targets\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiGziG15iykl"
      },
      "source": [
        "# make Validation\n",
        "df_valid = df[(df.date >= 20210401) & (df.date <= 20210430)]\n",
        "dts = TrainDataset(df_valid, df_valid.shape[0])\n",
        "\n",
        "df_dairy, df_submission, targets = next(dts)\n",
        "\n",
        "X_val = pre.pre_process(df_dairy, df_submission, phase='eval')\n",
        "df_targets = pd.concat(map(lambda z: unpack_json(z), targets), axis=0)\n",
        "df_targets['engagementMetricsDate'] = pd.to_datetime(df_targets['engagementMetricsDate'])\n",
        "y_val = pd.merge(X_val[['engagementMetricsDate', 'playerId']], df_targets, on=['engagementMetricsDate', 'playerId'], how='left')\n",
        "\n",
        "del df_targets"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfs_ChmjRlwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0a9602-25df-47d9-ee3a-170b6401e534"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQuEYiuyQ7Ia",
        "outputId": "bc3d96c4-9df3-41dc-b322-2b107d1f994b"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "params = {'objective': 'regression',\n",
        "             'metric': 'rmse',\n",
        "             'verbose': -1,\n",
        "             'feature_pre_filter': False,\n",
        "             'lambda_l1': 1.9246603611247695,\n",
        "             'lambda_l2': 0.0015207873611208637,\n",
        "             'num_leaves': 45,\n",
        "             'feature_fraction': 0.616,\n",
        "             'bagging_fraction': 1.0,\n",
        "             'bagging_freq': 0,\n",
        "             'min_child_samples': 20,\n",
        "}\n",
        "\n",
        "target_columns = ['target1', 'target2', 'target3', 'target4']\n",
        "\n",
        "models = []\n",
        "for target_column in target_columns:\n",
        "  lgb_train = lgb.Dataset(X.drop(['engagementMetricsDate'], axis=1), y[target_column])\n",
        "  lgb_valid = lgb.Dataset(X_val.drop(['engagementMetricsDate'], axis=1), y_val[target_column])\n",
        "  model = lgb.train(params, lgb_train, num_boost_round=200, valid_sets=lgb_valid, early_stopping_rounds=10)\n",
        "  models.append(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\tvalid_0's rmse: 6.10687\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[2]\tvalid_0's rmse: 6.10051\n",
            "[3]\tvalid_0's rmse: 6.09414\n",
            "[4]\tvalid_0's rmse: 6.08754\n",
            "[5]\tvalid_0's rmse: 6.08149\n",
            "[6]\tvalid_0's rmse: 6.07535\n",
            "[7]\tvalid_0's rmse: 6.07136\n",
            "[8]\tvalid_0's rmse: 6.07155\n",
            "[9]\tvalid_0's rmse: 6.06712\n",
            "[10]\tvalid_0's rmse: 6.06276\n",
            "[11]\tvalid_0's rmse: 6.06293\n",
            "[12]\tvalid_0's rmse: 6.05959\n",
            "[13]\tvalid_0's rmse: 6.05701\n",
            "[14]\tvalid_0's rmse: 6.05698\n",
            "[15]\tvalid_0's rmse: 6.05684\n",
            "[16]\tvalid_0's rmse: 6.05387\n",
            "[17]\tvalid_0's rmse: 6.05274\n",
            "[18]\tvalid_0's rmse: 6.05447\n",
            "[19]\tvalid_0's rmse: 6.0544\n",
            "[20]\tvalid_0's rmse: 6.05127\n",
            "[21]\tvalid_0's rmse: 6.05112\n",
            "[22]\tvalid_0's rmse: 6.04815\n",
            "[23]\tvalid_0's rmse: 6.04734\n",
            "[24]\tvalid_0's rmse: 6.04775\n",
            "[25]\tvalid_0's rmse: 6.04784\n",
            "[26]\tvalid_0's rmse: 6.04706\n",
            "[27]\tvalid_0's rmse: 6.04716\n",
            "[28]\tvalid_0's rmse: 6.04495\n",
            "[29]\tvalid_0's rmse: 6.04304\n",
            "[30]\tvalid_0's rmse: 6.04314\n",
            "[31]\tvalid_0's rmse: 6.04422\n",
            "[32]\tvalid_0's rmse: 6.0427\n",
            "[33]\tvalid_0's rmse: 6.04037\n",
            "[34]\tvalid_0's rmse: 6.03921\n",
            "[35]\tvalid_0's rmse: 6.03748\n",
            "[36]\tvalid_0's rmse: 6.03815\n",
            "[37]\tvalid_0's rmse: 6.03827\n",
            "[38]\tvalid_0's rmse: 6.03666\n",
            "[39]\tvalid_0's rmse: 6.03598\n",
            "[40]\tvalid_0's rmse: 6.03579\n",
            "[41]\tvalid_0's rmse: 6.03401\n",
            "[42]\tvalid_0's rmse: 6.03459\n",
            "[43]\tvalid_0's rmse: 6.03442\n",
            "[44]\tvalid_0's rmse: 6.03359\n",
            "[45]\tvalid_0's rmse: 6.03253\n",
            "[46]\tvalid_0's rmse: 6.03173\n",
            "[47]\tvalid_0's rmse: 6.03082\n",
            "[48]\tvalid_0's rmse: 6.03027\n",
            "[49]\tvalid_0's rmse: 6.02982\n",
            "[50]\tvalid_0's rmse: 6.03029\n",
            "[51]\tvalid_0's rmse: 6.03016\n",
            "[52]\tvalid_0's rmse: 6.03038\n",
            "[53]\tvalid_0's rmse: 6.03024\n",
            "[54]\tvalid_0's rmse: 6.03012\n",
            "[55]\tvalid_0's rmse: 6.0294\n",
            "[56]\tvalid_0's rmse: 6.02875\n",
            "[57]\tvalid_0's rmse: 6.02805\n",
            "[58]\tvalid_0's rmse: 6.02761\n",
            "[59]\tvalid_0's rmse: 6.02759\n",
            "[60]\tvalid_0's rmse: 6.02649\n",
            "[61]\tvalid_0's rmse: 6.0263\n",
            "[62]\tvalid_0's rmse: 6.02634\n",
            "[63]\tvalid_0's rmse: 6.02634\n",
            "[64]\tvalid_0's rmse: 6.02633\n",
            "[65]\tvalid_0's rmse: 6.02536\n",
            "[66]\tvalid_0's rmse: 6.02506\n",
            "[67]\tvalid_0's rmse: 6.02488\n",
            "[68]\tvalid_0's rmse: 6.02352\n",
            "[69]\tvalid_0's rmse: 6.02274\n",
            "[70]\tvalid_0's rmse: 6.02209\n",
            "[71]\tvalid_0's rmse: 6.02188\n",
            "[72]\tvalid_0's rmse: 6.01982\n",
            "[73]\tvalid_0's rmse: 6.01918\n",
            "[74]\tvalid_0's rmse: 6.01879\n",
            "[75]\tvalid_0's rmse: 6.0186\n",
            "[76]\tvalid_0's rmse: 6.01919\n",
            "[77]\tvalid_0's rmse: 6.01925\n",
            "[78]\tvalid_0's rmse: 6.01873\n",
            "[79]\tvalid_0's rmse: 6.01865\n",
            "[80]\tvalid_0's rmse: 6.01777\n",
            "[81]\tvalid_0's rmse: 6.01738\n",
            "[82]\tvalid_0's rmse: 6.01715\n",
            "[83]\tvalid_0's rmse: 6.01763\n",
            "[84]\tvalid_0's rmse: 6.01736\n",
            "[85]\tvalid_0's rmse: 6.01738\n",
            "[86]\tvalid_0's rmse: 6.01739\n",
            "[87]\tvalid_0's rmse: 6.01685\n",
            "[88]\tvalid_0's rmse: 6.01681\n",
            "[89]\tvalid_0's rmse: 6.01637\n",
            "[90]\tvalid_0's rmse: 6.01635\n",
            "[91]\tvalid_0's rmse: 6.01615\n",
            "[92]\tvalid_0's rmse: 6.01579\n",
            "[93]\tvalid_0's rmse: 6.01567\n",
            "[94]\tvalid_0's rmse: 6.01553\n",
            "[95]\tvalid_0's rmse: 6.0155\n",
            "[96]\tvalid_0's rmse: 6.0152\n",
            "[97]\tvalid_0's rmse: 6.01497\n",
            "[98]\tvalid_0's rmse: 6.01446\n",
            "[99]\tvalid_0's rmse: 6.01441\n",
            "[100]\tvalid_0's rmse: 6.01395\n",
            "[101]\tvalid_0's rmse: 6.01328\n",
            "[102]\tvalid_0's rmse: 6.01323\n",
            "[103]\tvalid_0's rmse: 6.01302\n",
            "[104]\tvalid_0's rmse: 6.01284\n",
            "[105]\tvalid_0's rmse: 6.01295\n",
            "[106]\tvalid_0's rmse: 6.01289\n",
            "[107]\tvalid_0's rmse: 6.01301\n",
            "[108]\tvalid_0's rmse: 6.01306\n",
            "[109]\tvalid_0's rmse: 6.01292\n",
            "[110]\tvalid_0's rmse: 6.01304\n",
            "[111]\tvalid_0's rmse: 6.01286\n",
            "[112]\tvalid_0's rmse: 6.01289\n",
            "[113]\tvalid_0's rmse: 6.01289\n",
            "[114]\tvalid_0's rmse: 6.01303\n",
            "Early stopping, best iteration is:\n",
            "[104]\tvalid_0's rmse: 6.01284\n",
            "[1]\tvalid_0's rmse: 7.82253\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[2]\tvalid_0's rmse: 7.80377\n",
            "[3]\tvalid_0's rmse: 7.78229\n",
            "[4]\tvalid_0's rmse: 7.76588\n",
            "[5]\tvalid_0's rmse: 7.76397\n",
            "[6]\tvalid_0's rmse: 7.75138\n",
            "[7]\tvalid_0's rmse: 7.7459\n",
            "[8]\tvalid_0's rmse: 7.74793\n",
            "[9]\tvalid_0's rmse: 7.75531\n",
            "[10]\tvalid_0's rmse: 7.74561\n",
            "[11]\tvalid_0's rmse: 7.7485\n",
            "[12]\tvalid_0's rmse: 7.74081\n",
            "[13]\tvalid_0's rmse: 7.73861\n",
            "[14]\tvalid_0's rmse: 7.73757\n",
            "[15]\tvalid_0's rmse: 7.739\n",
            "[16]\tvalid_0's rmse: 7.7383\n",
            "[17]\tvalid_0's rmse: 7.73512\n",
            "[18]\tvalid_0's rmse: 7.74219\n",
            "[19]\tvalid_0's rmse: 7.74403\n",
            "[20]\tvalid_0's rmse: 7.74205\n",
            "[21]\tvalid_0's rmse: 7.74301\n",
            "[22]\tvalid_0's rmse: 7.74115\n",
            "[23]\tvalid_0's rmse: 7.74004\n",
            "[24]\tvalid_0's rmse: 7.74025\n",
            "[25]\tvalid_0's rmse: 7.74116\n",
            "[26]\tvalid_0's rmse: 7.73989\n",
            "[27]\tvalid_0's rmse: 7.74137\n",
            "Early stopping, best iteration is:\n",
            "[17]\tvalid_0's rmse: 7.73512\n",
            "[1]\tvalid_0's rmse: 5.65624\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[2]\tvalid_0's rmse: 5.64985\n",
            "[3]\tvalid_0's rmse: 5.64239\n",
            "[4]\tvalid_0's rmse: 5.63597\n",
            "[5]\tvalid_0's rmse: 5.63619\n",
            "[6]\tvalid_0's rmse: 5.63088\n",
            "[7]\tvalid_0's rmse: 5.62755\n",
            "[8]\tvalid_0's rmse: 5.62775\n",
            "[9]\tvalid_0's rmse: 5.62852\n",
            "[10]\tvalid_0's rmse: 5.62499\n",
            "[11]\tvalid_0's rmse: 5.62533\n",
            "[12]\tvalid_0's rmse: 5.62397\n",
            "[13]\tvalid_0's rmse: 5.61991\n",
            "[14]\tvalid_0's rmse: 5.62007\n",
            "[15]\tvalid_0's rmse: 5.62048\n",
            "[16]\tvalid_0's rmse: 5.61977\n",
            "[17]\tvalid_0's rmse: 5.61901\n",
            "[18]\tvalid_0's rmse: 5.618\n",
            "[19]\tvalid_0's rmse: 5.61815\n",
            "[20]\tvalid_0's rmse: 5.61537\n",
            "[21]\tvalid_0's rmse: 5.61564\n",
            "[22]\tvalid_0's rmse: 5.61399\n",
            "[23]\tvalid_0's rmse: 5.61123\n",
            "[24]\tvalid_0's rmse: 5.61101\n",
            "[25]\tvalid_0's rmse: 5.61157\n",
            "[26]\tvalid_0's rmse: 5.60963\n",
            "[27]\tvalid_0's rmse: 5.60978\n",
            "[28]\tvalid_0's rmse: 5.60916\n",
            "[29]\tvalid_0's rmse: 5.60909\n",
            "[30]\tvalid_0's rmse: 5.60991\n",
            "[31]\tvalid_0's rmse: 5.60977\n",
            "[32]\tvalid_0's rmse: 5.60955\n",
            "[33]\tvalid_0's rmse: 5.60994\n",
            "[34]\tvalid_0's rmse: 5.60918\n",
            "[35]\tvalid_0's rmse: 5.60895\n",
            "[36]\tvalid_0's rmse: 5.60929\n",
            "[37]\tvalid_0's rmse: 5.60974\n",
            "[38]\tvalid_0's rmse: 5.60978\n",
            "[39]\tvalid_0's rmse: 5.60884\n",
            "[40]\tvalid_0's rmse: 5.60867\n",
            "[41]\tvalid_0's rmse: 5.60911\n",
            "[42]\tvalid_0's rmse: 5.60953\n",
            "[43]\tvalid_0's rmse: 5.60947\n",
            "[44]\tvalid_0's rmse: 5.60872\n",
            "[45]\tvalid_0's rmse: 5.60838\n",
            "[46]\tvalid_0's rmse: 5.6086\n",
            "[47]\tvalid_0's rmse: 5.60847\n",
            "[48]\tvalid_0's rmse: 5.6086\n",
            "[49]\tvalid_0's rmse: 5.6085\n",
            "[50]\tvalid_0's rmse: 5.60848\n",
            "[51]\tvalid_0's rmse: 5.6091\n",
            "[52]\tvalid_0's rmse: 5.60942\n",
            "[53]\tvalid_0's rmse: 5.60985\n",
            "[54]\tvalid_0's rmse: 5.60995\n",
            "[55]\tvalid_0's rmse: 5.60957\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's rmse: 5.60838\n",
            "[1]\tvalid_0's rmse: 6.28368\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[2]\tvalid_0's rmse: 6.2422\n",
            "[3]\tvalid_0's rmse: 6.21289\n",
            "[4]\tvalid_0's rmse: 6.19224\n",
            "[5]\tvalid_0's rmse: 6.17179\n",
            "[6]\tvalid_0's rmse: 6.15444\n",
            "[7]\tvalid_0's rmse: 6.13194\n",
            "[8]\tvalid_0's rmse: 6.13207\n",
            "[9]\tvalid_0's rmse: 6.11673\n",
            "[10]\tvalid_0's rmse: 6.10098\n",
            "[11]\tvalid_0's rmse: 6.10196\n",
            "[12]\tvalid_0's rmse: 6.08866\n",
            "[13]\tvalid_0's rmse: 6.07942\n",
            "[14]\tvalid_0's rmse: 6.0783\n",
            "[15]\tvalid_0's rmse: 6.08252\n",
            "[16]\tvalid_0's rmse: 6.07317\n",
            "[17]\tvalid_0's rmse: 6.06586\n",
            "[18]\tvalid_0's rmse: 6.05587\n",
            "[19]\tvalid_0's rmse: 6.05494\n",
            "[20]\tvalid_0's rmse: 6.0499\n",
            "[21]\tvalid_0's rmse: 6.05052\n",
            "[22]\tvalid_0's rmse: 6.04446\n",
            "[23]\tvalid_0's rmse: 6.0388\n",
            "[24]\tvalid_0's rmse: 6.03388\n",
            "[25]\tvalid_0's rmse: 6.03497\n",
            "[26]\tvalid_0's rmse: 6.02892\n",
            "[27]\tvalid_0's rmse: 6.02818\n",
            "[28]\tvalid_0's rmse: 6.02487\n",
            "[29]\tvalid_0's rmse: 6.02085\n",
            "[30]\tvalid_0's rmse: 6.02225\n",
            "[31]\tvalid_0's rmse: 6.01824\n",
            "[32]\tvalid_0's rmse: 6.01627\n",
            "[33]\tvalid_0's rmse: 6.01025\n",
            "[34]\tvalid_0's rmse: 6.00555\n",
            "[35]\tvalid_0's rmse: 6.00366\n",
            "[36]\tvalid_0's rmse: 6.00505\n",
            "[37]\tvalid_0's rmse: 6.00619\n",
            "[38]\tvalid_0's rmse: 6.00513\n",
            "[39]\tvalid_0's rmse: 6.00165\n",
            "[40]\tvalid_0's rmse: 6.00155\n",
            "[41]\tvalid_0's rmse: 6.00099\n",
            "[42]\tvalid_0's rmse: 6.00422\n",
            "[43]\tvalid_0's rmse: 6.00376\n",
            "[44]\tvalid_0's rmse: 6.00248\n",
            "[45]\tvalid_0's rmse: 6.00104\n",
            "[46]\tvalid_0's rmse: 6.0014\n",
            "[47]\tvalid_0's rmse: 6.00131\n",
            "[48]\tvalid_0's rmse: 5.99809\n",
            "[49]\tvalid_0's rmse: 5.99517\n",
            "[50]\tvalid_0's rmse: 5.99478\n",
            "[51]\tvalid_0's rmse: 5.99817\n",
            "[52]\tvalid_0's rmse: 5.99849\n",
            "[53]\tvalid_0's rmse: 6.00187\n",
            "[54]\tvalid_0's rmse: 6.00276\n",
            "[55]\tvalid_0's rmse: 6.00343\n",
            "[56]\tvalid_0's rmse: 6.00341\n",
            "[57]\tvalid_0's rmse: 6.00362\n",
            "[58]\tvalid_0's rmse: 6.00061\n",
            "[59]\tvalid_0's rmse: 6.0006\n",
            "[60]\tvalid_0's rmse: 5.99974\n",
            "Early stopping, best iteration is:\n",
            "[50]\tvalid_0's rmse: 5.99478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ySfF8lpQ62s",
        "outputId": "4e244166-1b55-4528-a5dc-1a8ec2a42f88"
      },
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  df_test = df[(df.date >= 20210401) & (df.date <= 20210430)]\n",
        "  iter_test = TrainDataset(df_valid, 5)\n",
        "  for df_dairy, df_submission, _ in iter_test:\n",
        "    print(df_dairy.shape)\n",
        "    print(df_submission.shape)\n",
        "    X = pre.pre_process(df_dairy, df_submission, phase='eval')\n",
        "    X = X.drop('engagementMetricsDate', axis=1)\n",
        "    # for (i, model) in enumerate(models):\n",
        "    #   df_submission[target_columns[i]] = model.predict(X)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 11)\n",
            "(5935, 6)\n",
            "(5, 11)\n",
            "(5935, 6)\n",
            "(5, 11)\n",
            "(5935, 6)\n",
            "(5, 11)\n",
            "(5935, 6)\n",
            "(5, 11)\n",
            "(5935, 6)\n",
            "(5, 11)\n",
            "(5935, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg-22E-FGBds"
      },
      "source": [
        "df_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d5-l6DNQ6ze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbXJqQyz8byL"
      },
      "source": [
        "if 'kaggle_secrets' in sys.modules:  # only run while on Kaggle\n",
        "    import mlb\n",
        "\n",
        "    env = mlb.make_env()\n",
        "    iter_test = env.iter_test()\n",
        "\n",
        "    for (test_df, sample_prediction_df) in iter_test:\n",
        "    \n",
        "        # Example: unpack a dataframe from a json column\n",
        "        today_games = unpack_json(test_df['games'].iloc[0])\n",
        "    \n",
        "        # Make your predictions for the next day's engagement\n",
        "        sample_prediction_df['target1'] = 100.00\n",
        "    \n",
        "        # Submit your predictions \n",
        "        env.predict(sample_prediction_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}